<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="The official page of the CoRL 2023 paper Affordance-Driven Next-Best-View Planning for Robotic Grasping.">
  <meta name="keywords" content="Grasp Synthesis, Neural SDF, Next-Best-View Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Affordance-Driven Next-Best-View Planning for Robotic Grasping</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-B0VFWCM6MX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-B0VFWCM6MX');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Affordance-Driven Next-Best-View Planning for Robotic Grasping</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xuechao Zhang<sup>*,†</sup>,</span>
              <!-- <a href="https://sszxc.net">Xuechao Zhang</a><sup>*,†</sup>,</span> -->
            <span class="author-block">
              Dong Wang<sup>†</sup>,</span>
            <span class="author-block">
              Sun Han<sup>*,†</sup>,
            </span>
            <span class="author-block">
              Weichuang Li<sup>†</sup>,
            </span>
            <span class="author-block">
              Bin Zhao<sup>†</sup>,
            </span>
            <span class="author-block">
              Zhigang Wang<sup>†</sup>,
            </span>
            <span class="author-block">
              Xiaoming Duan<sup>*</sup>,
            </span>
            <span class="author-block">
              Chongrong Fang<sup>*</sup>,
            </span>
            <span class="author-block">
              Xuelong Li<sup>†</sup>,
            </span>
            <span class="author-block">
              Jianping He<sup>*</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>†</sup>Shanghai Artificial Intelligence Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/main.pdf"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/7tfD1WdLFUw?si=ns4yug86VpJCVdym" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
          <img src="./static/images/teaser.jpg" class="interpolation-image" alt="Interpolate start reference image." />
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">AffordanCE-driven Next-Best-View planning policy (ACE-NBV)</span> is a method that aims to find a feasible grasp for target object via continuously observing scenes from new viewpoints.
      </h2>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Grasping occluded objects in cluttered environments is an essential component in complex robotic
            manipulation tasks. In
            this paper, we introduce an <span class="dnerf">AffordanCE-driven Next-Best-View planning policy
              (ACE-NBV)</span> that tries to find a feasible
            grasp for target object via continuously observing scenes from new viewpoints. This policy is motivated by
            the
            observation that the grasp affordances of an occluded object can be better-measured under the view when the
            view-direction are the same as the grasp view.
            Specifically, our method leverages the paradigm of novel view imagery to predict the grasps affordances
            under previously
            unobserved view, and select next observation view based on the highest imagined grasp quality of the target
            object.
          </p>
          <p>
            The experimental results in simulation and on a real robot demonstrate the effectiveness of the proposed
            affordance-driven next-best-view planning policy. Additional results, code, and videos of real robot
            experiments can be
            found in the supplementary materials.
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Method Overview</h2>
          <img src="./static/images/policy.jpg"  style="width: 50%;" class="interpolation-image" alt="Interpolate start reference image." />
          <div class="content has-text-justified">
            <p>
              We consider this active grasp problem: picking up an occluded target object in
              cluttered scenes using a robotic arm with an eye-in-hand depth camera.
              The target object is partly visible within the initial camera view field and a 3D
              bounding box is given to locate the target object.
              Our goal is to design a policy that moves the robotic arm to find a feasible grasp for the target object.
            </p>
          </div>
        <h2 class="title is-4">Network Architecture</h2>
        <img src="./static/images/overview.jpg" class="interpolation-image" alt="Interpolate start reference image." />
        <div class="content has-text-justified">
          <p>
            The input is a TSDF voxel field <b>M</b> obtained from the depth image. The
            upper branch predicts the grasp affordances for the target object and the lower branch synthesizes the depth image of
            different views, including the previously unseen views. Both branches share the same tri-plane feature volume
            <b>C</b>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Results</h2>
        <h2 class="title is-4">Simulated Experiments</h2>
        <img src="./static/images/sim_case1.jpg" class="interpolation-image"
        alt="Interpolate start reference image." />
        <img src="./static/images/sim_case2.jpg" class="interpolation-image" alt="Interpolate start reference image." />
        <p><br></p>
        <h2 class="title is-4">Real Robot Experiments</h2>
        <img src="./static/images/real_case1.jpg"   style="width: 70%;" class="interpolation-image" alt="Interpolate start reference image." />
        </div>
      </div>
    </div>
    <!--/ Results. -->
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We thanks the author of <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> for sharing this awesome website tempelate.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
